{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy Label Classification\n",
    "### Web scraping for Copenhagen buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Digging for data__: I found one public company in Denmark called [https://sparenergi.dk/](https://sparenergi.dk/) which tries to guide people make more sustainable choices on the energy sources they use at home.\n",
    "\n",
    "As a first step in this process, they provide [a page](https://sparenergi.dk/forbruger/vaerktoejer/find-dit-energimaerke) where you can insert an address and get the respective energy classification.\n",
    "\n",
    "After some more digging around, I found out that they also have [a map](https://sparenergi.dk/demo/addresses/map) where such a overview is accessible so I decided to try and see if I can scrape it.\n",
    "\n",
    "After a lot of digging, I found out what the parameters for the HTTP requests were (the process involved playing with my browser in 'Inspect' mode, opening the 'Networking' tab and searching for the relevant HTTP requests as I was playing with the map). The most important one was a geographical polygon that described the area for which the information is to be fetched. An example of how such an HTTP request would look like can be found in [curl_request.txt](curl_request.txt). To get to the python HTTP request format accepted by Python's [requests](https://pypi.org/project/requests/) module I used [https://curl.trillworks.com/](https://curl.trillworks.com/) which basically does just this: takes in a CURL request and spits out a Python version.\n",
    "\n",
    "So I defined a large enough polygon that should encapsulate the Municipality of Copenhagen and, in order to parallelize scrapring, I broke it down into a 200x200 grid. I then made a request for each of the individual polygons in this grid and combined the information into a single file.\n",
    "\n",
    "Here is the large polygon I used for this process: ![Image](data/img/ibm_cph_map.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from datetime import datetime\n",
    "import time\n",
    "import requests\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to try and parallelize the scraping as much as possible, since I wanted to avoid sending off a single request and waiting for the server to send the large response back (I was also skeptikal on whether my python setup would actually be able to process in one go).\n",
    "\n",
    "After a bit of googling, I found [this](https://www.jpytr.com/post/analysinggeographicdatawithfolium/) very nice approach of doing so which essentially constructs a NxN grid of a 'parent' poligon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Split the larger area for which we want to retrieve buildig energy classes into a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form the grid\n",
    "lower_left = [utils.REGION_LAT[0], utils.REGION_LON[0]]\n",
    "upper_right = [utils.REGION_LAT[2], utils.REGION_LON[2]]\n",
    "grid = utils.get_geojson_grid(upper_right, lower_left , n=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create the HTTP requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.001349\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "headers_l = list()\n",
    "cookies_l = list()\n",
    "data_l = list()\n",
    "\n",
    "for i in range(200):\n",
    "    h, c, d = utils.fill_request(grid[i])\n",
    "    headers_l.append(h), cookies_l.append(c), data_l.append(d)\n",
    "\n",
    "print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fire off the requests in a parallelized way and store the responses to file (one per process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(mp.cpu_count())\n",
    "results = pool.map(utils.make_request, [(cookies_l[i], headers_l[i], data_l[i]) for i in range(200)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
